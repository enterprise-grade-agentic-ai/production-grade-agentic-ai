# üß† Section 07. Agent Memory

## üéØ Purpose
This sections focuses on Agent Memory. Demos in the course included using `Short-Term` memory and `Long-Term` memory. We used short-term memory for conversation history and long-term memory for user preferences. This repo contains the code used for the demos.

## üõ†Ô∏è Installation
This project uses [UV](https://docs.astral.sh/uv/) for dependency management and package handling, offering a seamless setup and execution experience.

First, if you haven't already, install uv:
```bash
pip install uv
```

Next, navigate to your project directory and install the dependencies:
```bash
uv sync
```

## ‚úÖ Pre-requisites
___Instead of automating the pre-requisites, most of the steps are kept as manual. This has been intentionally done so that the learner can grasp the concepts in more detail.___
### AWS CLI Setup
___If not already done in the previous sections___, you may follow below steps to setup AWS CLI:
1. Install AWS CLI using [User Guide](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)
2. Configure AWS CLI using the below command on the terminal. Try to keep region as `us-east-1` as it generally gets the latest updates. 
```bash
aws configure 
```
Note: The AWS key or user used in `aws configure` command should have read and write permissions for AWS Bedrock, AgentCore Runtime, AgentCore Memory, AgentCore MCP, AWS Cognito, and AWS Secrets Manager.

### Env file setup
___If not already done in the previous sections___, you may copy `.env.template` as `.env` and populate it as mentioned in the subsequent steps.

### AWS Secrets Manager Setup (Optional)
We are talking about production grade agentic AI. Which means we shouldn't have secrets sprinkled in the local environment file or code. To ensure this security best practice, ideally you should create a secret in AWS secrets manager of type `Other type of secret`. You can populate secrets like `OpenAI key`, `Langfuse private key` in this container secret as key value pair. Key is the secret name, and value is the secret value. You needn't populate secrets in it as of now. AWS might stop you from saving the secret without any key value. You may save a dummy key value just to proceed.

After that set below variables in the `.env` file present in the current folder:
```python
SECRET_NAME=<Name of the secret as created in the AWS Secrets Manager>
SECRET_REGION=<AWS region where this secret is created, e.g. us-east-1>
```
The code is already in place to load the secrets from this container into environment file. Please note that secrets stored in AWS secrets manager costs. If you wouldn't like to incur that cost, you may set secrets in the local `.env` file. In that case, for security reasons, you should only try out the agentic application on local machine only.

### OpenAI Setup (Only if using OpenAI models)
:memo: This repo by default uses Amazon's Nova Pro model. But in my personal experience, OpenAI's gpt models perform better. You may consider setting up OpenAI for this repo if your budget allows.

___If not already done in the previous sections___, and you would like to use OpenAI models, you may follow below steps.
1. Generate OpenAI key if you would like to use OpenAI models. This can be done on [OpenAI Platform](https://platform.openai.com/). Please note that using OpenAI models would require minimum payment of $5 to OpenAI.
2. Set below key in the AWS secret containing secrets. This would be automatically loaded in the environment variables of the application. If you are not using AWS secret and just trying on you local, you may set this in `.env` file. 
```python
OPENAI_API_KEY=<Key as generated by OpenAI>
```

### Langfuse Setup (For observability)
___If not already done in the previous sections___, you may setup Langfuse for observability using the below steps:
1. Generate Langfuse keys on Langfuse.
2. Set below keys in the AWS secret containing secrets. This would be automatically loaded in the environment variables of the application. If you are not using AWS secret and just trying on local, you may set these in `.env` file. 
```
LANGFUSE_PUBLIC_KEY=<Public key as generated by Langfuse>
LANGFUSE_SECRET_KEY=<Secret key as generated by Langfuse>
LANGFUSE_BASE_URL=<Host as specified by Langfuse while generating the keys>
```

### AWS Cognito Setup
___If not already done in the previous sections___, you may setup cognito for authentication for MCP Gateway.

1. To setup AWS Cognito, you may run:
```bash
sh miscellaneous/cognitoSetup.sh 
```
Please note down Discovery URL as printed on the terminal.

### AWS AgentCore MCP Gateway Setup
___If not already done in the previous sections___, you may setup MCP Gateway as per the steps below:

1. To setup Client ID for MCP Gateway, you may go to `AWS Cognito Console -> User Pool Created in previous steps -> App Clients` and create an App Client of type `Machine-to-machine application`. You may note down `Client ID` and `Client Secret` of the newly created client ID for MCP Gateway. Store these values in the below environment variables or as key values in the AWS Secret.
```python
MCP_CLIENT_ID=<Client ID of the app client created for MCP Gateway>
MCP_CLIENT_SECRET=<Client Secret of the app client created for MCP Gateway>
```

2. Visit Cognito Discovery URL in a web browser, and pick up `token_endpoint` from the the JSON returned from the Discovery URL. Store below environment variable.
```python
MCP_TOKEN_URL=<token_endpoint as seen in the Discovery URL>
```

3. Create [Tavily](https://www.tavily.com/) account for web search. Create a API key in Tavily and copy it to clipboard. 

4. Visit AWS AgentCore Console and go to identity section, and add an API key with the following details:
```
Name: TavilyAPIKey
API key: <Tavily API Key created in previous step>
```

5. Create a MCP Gateway in AgentCore console using below configurations:
* Inbound Auth Type: `JWT`
* JWT schema configuration: `Use existing Identity provider configurations` . 
* Use Discovery URL as received from AWS Cognito
* Client value of `MCP_CLIENT_ID`
* Create a new target with below values:
  * Target Name: Tavily-Target
  * Target Type: Integrations
  * Integration Provider: Tavily
  * API Key: `<Key created in previous step>`

6. Copy the `Gateway resource URL` of the newly created MCP Gateway and set below environment variable in the `.env` file
```python
MCP_GATEWAY_URL=<Gateway resource URL>
```

### üÜï AWS Bedrock AgentCore Memory Setup (Mandatory since code relies on short-term and long-term memory)
___This is a new pre-requisite introduced in this section___. You may setup Agent Memory using the below steps:
1. Create a short-term memory with a small event expiration to save cost. Copy `Memory ID` from the newly created short term memory. Based on this, set below variable in `.env` file.
```python
MEMORY_ID=<Created Memory ID>
```
2. Add a long-term memory strategy for extracting user preferences from the short-term memory. Copy the `Strategy ID` from this newly created strategy. Based on this, set below environment variable in the `.env` file:
```python
MEMORY_STRATEGY_ID=<Created Strategy ID>
```

## üöÄ Running the Project
:memo: It is a good idea to observe every execution trace in Langfuse. If LLM call isn't visible in the trace, try seeing the output of the agents in agent spans. It should have response returned from LLMs. 

### Short-Term memory
Code for memory is in `src/emergingtechnologyresearch/flow.py` and `src/emergingtechnologyresearch/utils/memoryUtils.py`. You may follow below steps to try out short-term memory:

1. Ensure that AWS Bedrock AgentCore Memory is setup as mentioned earlier in this readme.
2. Run the chat interface using the below command:
```bash
ACTOR_ID="John" uv run python -m src.emergingtechnologyresearch.chat
```
3. Ask the application to research on a topic and generate a report. This should ideally trigger research crew and also store the response in short-term memory.
4. Ask a followup question to the application. The response to the followup question will be made using history stored in short-term memory. You may explore the execution trace in Langfuse to see how conversation history is made available to the followupCrew.

### Long-Term memory
You may follow below steps to try out long-term memory:
1. Please ensure that you have first tried out short-term memory. This would make sure that preferences will be extracted from short-term memory and stored in long-term memory.
2. Run the chat interface using the below command:
```bash
ACTOR_ID="John" uv run python -m src.emergingtechnologyresearch.chat
```
3. Ask the application to research on a topic and generate a report. You may explore the execution trace in Langfuse and see how preferences formed from the last execution are available to this execution as well.

_Note: Sometimes extraction of long-term memory takes a few minutes. But generally it is done in a few seconds_


**Happy Learning! üéâü§ñ**