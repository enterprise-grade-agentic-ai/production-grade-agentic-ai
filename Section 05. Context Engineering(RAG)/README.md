# üìö Section 05. Context Engineering (RAG)

## üéØ Purpose
This sections focuses on Context Engineering and RAG. Demos in the course included building `AWS Knowledge Base` using S3 as the data source and pinecone vector database as the destination. This AWS Knowledge Base was made available to our CrewAI application using Agentic RAG. This repo contains the code used for the demos.

## üõ†Ô∏è Installation
This project uses [UV](https://docs.astral.sh/uv/) for dependency management and package handling, offering a seamless setup and execution experience.

First, if you haven't already, install uv:
```bash
pip install uv
```

Next, navigate to your project directory and install the dependencies:
```bash
uv sync
```

## ‚úÖ Pre-requisites
___Instead of automating the pre-requisites, most of the steps are kept as manual. This has been intentionally done so that the learner can grasp the concepts in more detail.___

### AWS CLI Setup
___If not already done in the previous sections___, you may follow below steps to setup AWS CLI:
1. Install AWS CLI using [User Guide](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)
2. Configure AWS CLI using the below command on the terminal. Try to keep region as `us-east-1` as it generally gets the latest updates.
```bash
aws configure 
```
Note: The AWS key or user used in `aws configure` command should have read and write permissions for AWS Bedrock, AgentCore Runtime, AgentCore Memory, AgentCore MCP, AWS Cognito, and AWS Secrets Manager.

### Env file setup
___If not already done in the previous sections___, you may copy `.env.template` as `.env` and populate it as mentioned in the subsequent steps.

### AWS Secrets Manager Setup (Optional)
We are talking about production grade agentic AI. Which means we shouldn't have secrets sprinkled in the local environment file or code. To ensure this security best practice, ideally you should create a secret in AWS secrets manager of type `Other type of secret`. You can populate secrets like `OpenAI key`, `Langfuse private key` in this container secret as key value pair. Key is the secret name, and value is the secret value. You needn't populate secrets in it as of now. AWS might stop you from saving the secret without any key value. You may save a dummy key value just to proceed.

After that set below variables in the `.env` file present in the current folder:
```python
SECRET_NAME=<Name of the secret as created in the AWS Secrets Manager>
SECRET_REGION=<AWS region where this secret is created, e.g. us-east-1>
```
The code is already in place to load the secrets from this container into environment file. Please note that secrets stored in AWS secrets manager costs. If you wouldn't like to incur that cost, you may set secrets in the local `.env` file. In that case, for security reasons, you should only try out the agentic application on local machine only.

### OpenAI Setup (Only if using OpenAI models)
:memo: This repo by default uses Amazon's Nova Pro model. But in my personal experience, OpenAI's gpt models perform better. You may consider setting up OpenAI for this repo if your budget allows.

___If not already done in the previous sections___, and you would like to use OpenAI models, you may follow below steps.
1. Generate OpenAI key if you would like to use OpenAI models. This can be done on [OpenAI Platform](https://platform.openai.com/). Please note that using OpenAI models would require minimum payment of $5 to OpenAI.
2. Set below key in the AWS secret containing secrets. This would be automatically loaded in the environment variables of the application. If you are not using AWS secret and just trying on you local, you may set this in `.env` file. 
```python
OPENAI_API_KEY=<Key as generated by OpenAI>
```

### Langfuse Setup (For observability)
___If not already done in the previous sections___, you may setup Langfuse for observability using the below steps:
1. Generate Langfuse keys on Langfuse.
2. Set below keys in the AWS secret containing secrets. This would be automatically loaded in the environment variables of the application. If you are not using AWS secret and just trying on local, you may set these in `.env` file. 
```
LANGFUSE_PUBLIC_KEY=<Public key as generated by Langfuse>
LANGFUSE_SECRET_KEY=<Secret key as generated by Langfuse>
LANGFUSE_BASE_URL=<Host as specified by Langfuse while generating the keys>
```
### üÜï Setup Pinecone as the vector database
___This is a new pre-requisite introduced in this section___. You may setup Pinecone as per the steps below:
1. Sign up on [Pinecone](https://www.pinecone.io/) using a free plan.
2. Create an index with following details:
* Index Name: `emerging-technology-research`
* Configuration: `Custom settings`
  * Dimension: `1024`
  * Metric: `Cosine`
3. Copy the Host URL of the newly created pinecone index. This will be used later.
4. Create an API key in pinecone and copy it for the next step.
5. Create a secret in AWS secrets manager to store this API key. Please note that this is a mandatory step, since AWS Knowledge Base requires it this way. You may use below attributes while creating the secret:
* Secret Type: `Other type of secret`
* Key: `apikey`
* Value: `<API of pinecone as created in the previous step>`
* Secret Name: `emergingtechnologyresearch/pinecone`
6. You may copy the ARN of this secret. This will be required later.

### üÜï Setup AWS S3 bucket containing knowledge documents (Will act as data source for AWS Knowledge Base)
___This is a new pre-requisite introduced in this section___. You may setup AWS S3 bucket as per the steps below:
1. Create a S3 bucket which will contain the PDFs which will act as a data source for the knowledge base.
2. Create a folder with the name `rag` inside the bucket
3. Copy the files in `miscellaneous/documentsForRAG` folder in this repo to the newly created folder in the previous step.

### üÜï Setup AWS Knowledge Base as the knowledge base
___This is a new pre-requisite introduced in this section___. You may setup AWS Knowledge Base as per the steps below:
1. Visit AWS Bedrock console and navigate to `Knowledge Bases`. Create a `Knowledge Base with vector store` using below attributes:
* Knowledge Base Details:
  * Name: `emergingtechnologyresearch-knowledgebase`
  * Runtime Role: `Create and use a new service role`
  * Choose data source type: `Amazon S3`
* Configure data source
  * S3 URI: Select the S3 bucket created earlier, and choose the folder with the name `rag`
  * Parsing strategy: `Amazon Bedrock default parser`
  * Chunking strategy: `Default chunking`
* Configure data storage and processing
  * Embedding Model: `Titan Text Embeddings V2`
  * Vector store: `Use an existing vector store`
  * Vector store type: `Pinecone`
  * Endpoint URL: `<Host URL as copied from pinecone index>`
  * Credentials secret ARN: `<ARN of the AWS Secret containing the pinecone API key>`
  * Text field name: `text`
  * Bedrock-managed metadata field name: `metadata`
2. In the newly created AWS Knowledge Base, you may select the data source and synchronize vector data store with the data source using the `sync` button. Once this is completed, you may explore the index in pinecone. It should have roughly 185 records for the two documents we uploaded in the data source. You may explore couple of records on how those are chunked.
3. Now you may copy the `Knowledge Base ID` of the newly created AWS Knowledge Base and store it in the below environment variable in the `.env` file:
```
AWS_KNOWLEDGE_BASE_ID=<Knowledge Base ID as copied in the previous step>
```

## üöÄ Running the Project
:memo: It is a good idea to observe every execution trace in Langfuse. If LLM call isn't visible in the trace, try seeing the output of the agents in agent spans. It should have response returned from LLMs. 

To see Agentic RAG in action, you may use below steps:
1. Run the CLI interface using the below command:
```bash
uv run python -m src.emergingtechnologyresearch.run
```
2. When asked by CLI, you may try out a topic for which we have data in AWS Knowledge Base, e.g. Quantum Computing. Post the execution you may explore the execution trace in Langfuse.
3. You may now re-run the CLI interface, and try out a topic for which we do not have data in AWS Knowledge Base, e.g. Satellite Technology. Post the execution you may explore the execution trace in Langfuse, and also explore if there are any differences from the previous execution.

**Happy Learning! üéâü§ñ**